#  层层递进，决策树模型

## 概念

**决策树类似我们人的决策过程。**比如买北京学区房。就读学校是什么？预算？面积？

决策树算法利用树形结构，使用层层递进提问得到最终的分类。通过上面的问题，我们就构建了一套买房的策略。

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210328170928.svg" alt="决策树" style="zoom: 67%;" />

* 每个非叶节点表示一个特征属性测试。
* 每个分支代表这个特征属性在某个值域（分类）上的输出。
* 每个叶子节点存放一个类别。
* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。

## 构造决策树

决策树包含根节点、内部节点和叶节点，其根节点包含样本全集，内部节点对应特征属性测试，叶节点则代表决策结果。

由于决策树是基于特征对实例进行分类的，因而其学习的本质是从训练数据集中归纳出一组用于分类的 if-else 规则。在学习的过程中，这组规则集合既要在训练数据上降低经验误差，也要具备良好的泛化能力（降低泛化误差）。

决策树模型的学习过程包括三个步骤：**特征选择、决策树生成、决策树剪枝**。

### 特征选择

**特征选择决定了使用哪些特征来划分特征空间**。训练集中，每个样本的属性可能有很多个，在分类结果中起到的作用也有大有小。因此特征选择作用在于筛选出与分类结果相关性较高，也就是分类能力较强的特征。理想的特征选择是在每次划分之后，分支节点所包含的样本都尽可能属于同一个类别。

> 
>
> 选择题有选项 A/B/C/D
>
> H = -log2(1/4)
>
> 「正确答案是 A 」信息量是 (-log2(4)) - (-log2(1/4))
>
> 「D一定是错误答案」信息量是 log2(1/4)
>
> 
>
> **获取信息就是消除不确定性**
>
> **信息量 = 不确定性减少量**
>
> https://zhuanlan.zhihu.com/p/26486223

**在特征选择中通常使用的准则是信息增益**。信息增益描述的是在已知特征后对数据分类不确定性的减少程度，因而特征的信息增益越大，得到的分类结果的不确定度越低，特征也就具有越强的分类能力。根据信息增益准则选择特征的过程，就是自顶向下进行划分，在每次划分时计算每个特征的信息增益并选取最大值的过程。信息增益的计算涉及信源熵和条件熵的公式。

如果事件 X 发生的概率是 p(X)，这个事件的自信息量的定义为
$$
\mathrm{H}(X) = -log_2(p(X))
$$


